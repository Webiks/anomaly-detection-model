# -*- coding: utf-8 -*-
"""monitor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18lyhgfI2_upDlN2ww8cXI9h-UcX_KFXl
"""

# from google.colab import files
# uploaded = files.upload()

from google.colab import drive
drive.mount('/content/drive')

#import
from google.colab import drive
import json
import pandas as pd
from pandas.io.json import json_normalize
import os 

from sklearn.ensemble import IsolationForest as IF
import pickle

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#import datetime
#from sklearn.preprocessing import OneHotEncoder as oh

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# load data
path=os.path.join('/content/drive/My Drive/monitor_data', 'test11.json')
X11 =json.load(open(path))
# path=os.path.join('/content/drive/My Drive/monitor_data', 'test12.json')
# X12=json.load(open(path))
X11=json_normalize(X11)
# X12=json_normalize(X12)

#for now we are trubbling loading and training big data - need to be solved in the future

#X=pd.concat([X11,X12],ignore_index=True)
X0=X11[0:100000]

#define functions

#mode function for text features
get_max = lambda x: x.value_counts(dropna=True).index[0]; get_max.__name__ = "most frequent"

# moving mean - not used for now
# def moving_mean(x, N):
#     cumsum = numpy.cumsum(numpy.insert(x, 0, 0)) 
#     return (cumsum[N:] - cumsum[:-N]) / float(N)

# percentile functions , quartile difference
def percentile_90(x):
        return x.quantile(0.9)
def percentile_75(x):
        return x.quantile(0.75)
def percentile_25(x):
        return x.quantile(0.25)
def percentile_10(x):
        return x.quantile(0.1)
def iqr(x):
    return x.quantile(0.75)-x.quantile(0.25)
def scatter(x):
    if x.mean()!=0:
        scatter=(x.mean()-x.median())/x.mean()
    else:
        'mean 0'
    return scatter

# processing data - create hostfingerprint and setting timestamp' 

X0['timestamp'] = pd.to_datetime(X0['@timestamp'])
X0['host_fingerprint']=X0['host.architecture']+'_'+X0['host.os.kernel']+'_'+X0['host.os.name']

# define lists of numerical and categorical features for processing and aggregation functions
stats_list=['sum', 'mean','std','median',percentile_25,percentile_75,iqr,percentile_10,percentile_90]
col_num=['timestamp','score','event.duration','system.cpu.softirq.pct','system.cpu.user.pct','system.cpu.total.pct','system.cpu.irq.pct','system.cpu.cores','system.cpu.nice.pct','system.cpu.idle.pct','system.cpu.system.pct','system.cpu.steal.pct','system.cpu.iowait.pct','system.memory.total','system.memory.used.bytes','system.memory.used.pct','system.memory.actual.used.pct','system.memory.actual.used.bytes','system.memory.actual.free','system.memory.swap.total','system.memory.swap.used.bytes','system.memory.swap.used.pct','system.memory.swap.free','system.memory.free','system.process.cpu.total.value','system.process.cpu.total.pct','system.process.cpu.total.norm.pct','system.process.memory.rss.pct','system.process.memory.rss.bytes','system.process.memory.size','system.process.memory.share','process.pgid','process.pid','process.ppid','system.socket.summary.tcp.all.established','system.socket.summary.tcp.all.close_wait','system.socket.summary.tcp.all.listening','system.socket.summary.tcp.all.count','system.socket.summary.tcp.all.time_wait','system.socket.summary.udp.all.count','system.socket.summary.all.listening','system.socket.summary.all.count','system.network.out.errors','system.network.out.bytes','system.network.out.packets','system.network.out.dropped','system.network.in.errors','system.network.in.bytes','system.network.in.packets','system.network.in.dropped','system.process.summary.total','system.process.summary.zombie','system.process.summary.stopped','system.process.summary.sleeping','system.process.summary.idle','system.process.summary.dead','system.process.summary.unknown','system.process.summary.running','system.filesystem.total','system.filesystem.files','system.filesystem.free_files','system.filesystem.available','system.filesystem.used.pct','system.filesystem.used.bytes','system.filesystem.free','system.fsstat.total_files','system.fsstat.total_size.used','system.fsstat.total_size.total','system.fsstat.total_size.free','system.fsstat.count','system.uptime.duration.ms','metricset.period']
col_category=['timestamp','event.dataset','metricset.name','user.name','process.name','system.network.name','host_fingerprint']

df_num=X0[col_num].set_index('timestamp').resample('T').agg(stats_list)

list(df_num.columns)

# drop constant columns - need to ask ourselves if it is right to do? or it may indicate for an error? 
train=df_num.loc[:, (df_num != df_num.iloc[0]).any()]
# drop NA columns - need to deal with NA - this is temporary solution for runing the model
train=train.dropna(1)


print('train shape ', train.shape, 'original df shape ', df_num.shape,'/n',train.head(10))

#model

clf=IF(behaviour='new',max_samples=10000,random_state=1,contamination=0.01)

preds=clf.fit_predict(train)
#print(list(preds).count(-1))
metrics_df=pd.DataFrame()
metrics_df['anomaly']=preds
outliers=metrics_df.loc[metrics_df['anomaly']==-1]
outlier_index=list(outliers.index)
#print(outlier_index)
#Find the number of anomalies and normal points here points classified -1 are anomalous
print(metrics_df['anomaly'].value_counts())

print(outlier_index)

# plot anomalies on 3d using pca
pca = PCA(n_components=3)  # Reduce to k=3 dimensions
scaler = StandardScaler()

#normalize the metrics
X = scaler.fit_transform(train)
X_reduce = pca.fit_transform(X)

fig = plt.figure(figsize=(20,10))
ax = fig.add_subplot(111, projection='3d')

ax.set_zlabel("x_composite_3")
# Plot the compressed data points
ax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=10, lw=1, label="inliers",c="green")
# Plot x's for the ground truth outliers
ax.scatter(X_reduce[outlier_index,0],X_reduce[outlier_index,1], X_reduce[outlier_index,2],
           lw=2, s=60, marker="x", c="red", label="outliers")
# for angle in range(0, 360):
#     ax.view_init(10, 10)
#     plt.draw()
#     plt.pause(10)
ax.legend()
plt.show()

from sklearn import preprocessing
data_scaled = pd.DataFrame(preprocessing.scale(train),columns = train.columns) 
data_scaled.head(10)
pca = PCA(n_components=3)
pca.fit_transform(data_scaled)
export_data=pd.DataFrame(pca.components_,columns=data_scaled.columns,index = ['PC-1','PC-2','PC-3'])


#data_scaled = pd.DataFrame(train.StandardScaler(),columns = train.columns)
#data_scaled=pd.DataFrame(X_reduce,columns=(train.columns))
# print (pd.DataFrame(pca.components_,columns=data_scaled.columns,index = ['PC-1','PC-2','PC-3']))

# print (pd.DataFrame(pca.components_,columns=X_reduce.columns,index = ['PC-1','PC-2','PC-3']))

from google.colab import files
export_data.to_csv('export_data.csv')



"""# New Section"""

X[col_category].head(10)

df_category_numeric_index=pd.DataFrame()
df_category_numeric=pd.DataFrame()
for column in X[col_category]:
  df_category_numeric_index[column]=pd.factorize(X[col_category][column])
  df_category_numeric[column]=pd.factorize(X[col_category][column])[0]

col_cat=col_category
print(col_cat)

df_category_numeric_index.iloc[1]

##CONVERTING CATEGORICAL FETURES TO NUMBERS
df_category=X['metricset.name'].set_index('@timestamp').resample('T').agg([get_max])

print(X11['@timestamp'].max(),X11['@timestamp'].min())
print(X12['@timestamp'].max(),X12['@timestamp'].min())

#!rm test3.json

from sklearn.ensemble import IsolationForest as IF
import pickle
import pandas as pd
import matplotlib.pyplot as plt
import json

#with open (test.json) as json_file: X=json.load(json_file)
#X=json_normalize(X)
X=json.loads(uploaded['test6.json'])
from pandas.io.json import json_normalize
X=json_normalize(X)

import pandas_profiling as pdp
pdp.ProfileReport(X)

X.head

#tempX=X.loc[X['_source.metricset.name']=='process']
#tempX['_source.process.name'].isnull().sum()
tempX=X.loc[X['_source.process.name'].isnull()==True]
tempX.groupby(tempX['_source.metricset.name']).first()
#df.loc[df.set_of_numbers <= 4, 'equal_or_lower_than_4?']