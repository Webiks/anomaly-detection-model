# -*- coding: utf-8 -*-
"""monitor 10112019.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zNNagepx6tdRkW9egs1tMf_Grvwyu4PN
"""

# from google.colab import files
# uploaded = files.upload()

from google.colab import drive
drive.mount('/content/drive')

#import
from google.colab import drive
import json
import pandas as pd
from pandas.io.json import json_normalize
import os 
from math import pi

from sklearn.ensemble import IsolationForest as IF
import pickle

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#import datetime
#from sklearn.preprocessing import OneHotEncoder as oh

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# load data -
######$$$$$$$$$$$$$$$$ need to be connected directly to logstash 

data_json= 'aws3.json'
path=os.path.join('/content/drive/My Drive/monitor_data', data_json)
print(path)
X=json.load(open(path))
#X11 =json.load(open(path))
# path=os.path.join(r'/content/drive/My Drive/monitor_data', 'test12.json')
# X12=json.load(open(path))
X=json_normalize(X)
# X12=json_normalize(X12)

#problem loading and training big data - need to be solved in the future

#X=pd.concat([X11,X12],ignore_index=True)
X0=X[0:100000]
print(X.shape)

#define functions

#mode function for text features
get_max = lambda x: x.value_counts(dropna=True).index[0]; get_max.__name__ = "most frequent"

# moving mean - not used for now
# def moving_mean(x, N):
#     cumsum = numpy.cumsum(numpy.insert(x, 0, 0)) 
#     return (cumsum[N:] - cumsum[:-N]) / float(N)

# percentile functions , quartile difference
def percentile_90(x):
        return x.quantile(0.9)
def percentile_75(x):
        return x.quantile(0.75)
def percentile_25(x):
        return x.quantile(0.25)
def percentile_10(x):
        return x.quantile(0.1)
def iqr(x):
    return x.quantile(0.75)-x.quantile(0.25)
def scatter(x):
    if x.mean()!=0:
        scatter=(x.mean()-x.median())/x.mean()
    else:
        'mean 0'
    return scatter

import pandas_profiling as pdp
pdp.ProfileReport(X)

# processing data - create hostfingerprint and setting timestamp' 

X0['timestamp'] = pd.to_datetime(X0['@timestamp'])
# X0['host_fingerprint']=X0['host.architecture']+'_'+X0['host.os.kernel']+'_'+X0['host.os.name']
# X0[['host_fingerprint','host.architecture','host.os.kernel','host.os.name']].head()

linux_n_features=['sort', 'doc_type', 'cloud.region', 'cloud.instance.id', 'cloud.machine.type', 'cloud.account.id', 'cloud.provider', 'cloud.image.id', 'cloud.availability_zone', 'host.os.codename', 'host.containerized', 'system.load.1',
'system.load.5', 'system.load.cores', 'system.load.norm.1', 'system.load.norm.5', 'system.load.norm.15', 'system.load.15', 'process.working_directory', 'process.executable', 'system.process.fd.open', 'system.process.fd.limit.soft', 'system.process.fd.limit.hard',
'system.process.cgroup.memory.kmem.limit.bytes', 'system.process.cgroup.memory.kmem.usage.bytes', 'system.process.cgroup.memory.kmem.usage.max.bytes', 'system.process.cgroup.memory.kmem.failures', 'system.process.cgroup.memory.kmem_tcp.limit.bytes',
'system.process.cgroup.memory.mem.failures', 'system.process.cgroup.memory.path', 'system.process.cgroup.memory.id', 'system.process.cgroup.memory.stats.hierarchical_memory_limit.bytes', 'system.process.cgroup.memory.stats.active_file.bytes',
'system.process.cgroup.memory.stats.mapped_file.bytes', 'system.process.cgroup.memory.stats.rss.bytes', 'system.process.cgroup.memory.stats.pages_in', 'system.process.cgroup.memory.stats.pages_out', 'system.process.cgroup.memory.stats.inactive_file.bytes',
'system.process.cgroup.memory.stats.inactive_anon.bytes', 'system.process.cgroup.memory.stats.active_anon.bytes', 'system.process.cgroup.memory.stats.unevictable.bytes', 'system.process.cgroup.memory.stats.cache.bytes', 'system.process.cgroup.memory.stats.swap.bytes',
'system.process.cgroup.memory.stats.page_faults', 'system.process.cgroup.memory.stats.hierarchical_memsw_limit.bytes', 'system.process.cgroup.memory.stats.rss_huge.bytes', 'system.process.cgroup.memory.stats.major_page_faults',
'system.process.cgroup.memory.memsw.limit.bytes', 'system.process.cgroup.memory.memsw.usage.bytes', 'system.process.cgroup.memory.memsw.usage.max.bytes', 'system.process.cgroup.memory.memsw.failures', 'system.process.cgroup.cpu.path',
'system.process.cgroup.cpu.cfs.shares', 'system.process.cgroup.cpu.cfs.period.us', 'system.process.cgroup.cpu.cfs.quota.us', 'system.process.cgroup.cpu.id', 'system.process.cgroup.cpu.stats.periods', 'system.process.cgroup.cpu.stats.throttled.periods',
'system.process.cgroup.cpu.stats.throttled.ns', 'system.process.cgroup.cpu.rt.period.us', 'system.process.cgroup.cpu.rt.runtime.us', 'system.process.cgroup.cpuacct.path', 'system.process.cgroup.cpuacct.total.ns', 'system.process.cgroup.cpuacct.id',
'system.process.cgroup.cpuacct.stats.system.ns', 'system.process.cgroup.cpuacct.stats.user.ns', 'system.process.cgroup.path', 'system.process.cgroup.id', 'system.process.cgroup.blkio.path', 'system.process.cgroup.blkio.total.ios',
'system.process.cgroup.blkio.total.bytes','system.process.cgroup.blkio.id','system.memory.hugepages.used.bytes','system.memory.hugepages.used.pct','system.memory.hugepages.free','system.memory.hugepages.reserved','system.memory.hugepages.surplus',
'system.memory.hugepages.total','system.memory.hugepages.default_size','system.diskio.read.bytes','system.diskio.read.time','system.diskio.read.count', 'system.diskio.write.bytes', 'system.diskio.write.count', 'system.diskio.write.time', 'system.diskio.name',
'system.diskio.io.time', 'system.diskio.iostat.read.per_sec.bytes', 'system.diskio.iostat.read.request.per_sec', 'system.diskio.iostat.read.request.merges_per_sec', 'system.diskio.iostat.read.await', 'system.diskio.iostat.write.per_sec.bytes',
'system.diskio.iostat.write.request.per_sec', 'system.diskio.iostat.write.request.merges_per_sec', 'system.diskio.iostat.write.await', 'system.diskio.iostat.request.avg_size', 'system.diskio.iostat.queue.avg_size', 'system.diskio.iostat.await',
'system.diskio.iostat.busy', 'system.diskio.iostat.service_time', 'system.socket.summary.udp.memory', 'system.socket.summary.tcp.all.orphan', 'system.socket.summary.tcp.memory']
X0=X0.drop(linux_n_features, axis = 1) 
X0 = X0[X0.columns.drop(list(X0.filter(regex='process')))]
list(X0.columns)

#X0[['hardware_fingerprint','system.cpu.cores','system.fsstat.total_size.total','system.filesystem.total','system.memory.total']].head()
# # define lists of numerical and categorical features for processing and aggregation functions
stats_list=['sum', 'mean','std','median',percentile_25,percentile_75,iqr,percentile_10,percentile_90]
# col_num=['timestamp','score','event.duration','system.cpu.softirq.pct','system.cpu.user.pct','system.cpu.total.pct','system.cpu.irq.pct','system.cpu.cores','system.cpu.nice.pct','system.cpu.idle.pct','system.cpu.system.pct','system.cpu.steal.pct','system.cpu.iowait.pct','system.memory.total','system.memory.used.bytes','system.memory.used.pct','system.memory.actual.used.pct','system.memory.actual.used.bytes','system.memory.actual.free','system.memory.swap.total','system.memory.swap.used.bytes','system.memory.swap.used.pct','system.memory.swap.free','system.memory.free','system.process.cpu.total.value','system.process.cpu.total.pct','system.process.cpu.total.norm.pct','system.process.memory.rss.pct','system.process.memory.rss.bytes','system.process.memory.size','system.process.memory.share','process.pgid','process.pid','process.ppid','system.socket.summary.tcp.all.established','system.socket.summary.tcp.all.close_wait','system.socket.summary.tcp.all.listening','system.socket.summary.tcp.all.count','system.socket.summary.tcp.all.time_wait','system.socket.summary.udp.all.count','system.socket.summary.all.listening','system.socket.summary.all.count','system.network.out.errors','system.network.out.bytes','system.network.out.packets','system.network.out.dropped','system.network.in.errors','system.network.in.bytes','system.network.in.packets','system.network.in.dropped','system.process.summary.total','system.process.summary.zombie','system.process.summary.stopped','system.process.summary.sleeping','system.process.summary.idle','system.process.summary.dead','system.process.summary.unknown','system.process.summary.running','system.filesystem.total','system.filesystem.files','system.filesystem.free_files','system.filesystem.available','system.filesystem.used.pct','system.filesystem.used.bytes','system.filesystem.free','system.fsstat.total_files','system.fsstat.total_size.used','system.fsstat.total_size.total','system.fsstat.total_size.free','system.fsstat.count','system.uptime.duration.ms','metricset.period']
# col_category=['timestamp','event.dataset','metricset.name','user.name','process.name','system.network.name','host_fingerprint']

print(X0.dtypes)
print(len(list(X0.columns)))

import pandas_profiling as pdp
pdp.ProfileReport(X0)

X0_object=X0.select_dtypes(include=['object'])
X0_object.head(20)

# setting agent hostname & creating hardware_fingerprint based on that - in the future hardwarefingerprint need to be based on actual hardware difference
X0['hardware_fingerprint']=X0['system.cpu.cores'].astype(str)+'_'+X0['system.fsstat.total_size.total'].astype(str)+'_'+X0['system.filesystem.total'].astype(str)+X0['system.memory.total'].astype(str)
X0['agent.hostname'] = X0['agent.hostname'].str.replace('stress1','1')
X0['agent.hostname'] = X0['agent.hostname'].str.replace('stress2','2')
X0['agent.hostname'] = X0['agent.hostname'].str.replace('stress3','3')
X0['agent.hostname'] = X0['agent.hostname'].str.replace('stress4','4')

X0['agent.hostname']=X0['agent.hostname'].astype(int)
# X0['agent.hostname'].dtypes
# X0['hardware_fingerprint'] = '1' if X0[(X0['agent.hostname']<3) else '2'
X0['hardware_fingerprint']=X0['agent.hostname']
X0['hardware_fingerprint']=X0['hardware_fingerprint'].replace(2,1)
X0['hardware_fingerprint']=X0['hardware_fingerprint'].replace([3,4],2)

# X0['hardware_fingerprint']=X0['agent.hostname']
# X0['hardware_fingerprint'].replace([1, 2], 1)
# X0['hardware_fingerprint'].replace([3, 4], 2)

print(X0[['agent.hostname','hardware_fingerprint']].head(10))
print(X0['hardware_fingerprint'].dtypes)

df_num=X0.select_dtypes(exclude=['object'])
print(list(df_num.columns))

# df_num.set_index('timestamp', inplace=True)
df_num.head()

df_num_new1=df_num[df_num['agent.hostname']== 1]
df_num_new2=df_num[df_num['agent.hostname']== 2]
df_num_new3=df_num[df_num['agent.hostname']== 3]
df_num_new4=df_num[df_num['agent.hostname']== 4]
df_num_new['timestamp'] = pd.to_datetime(df_num_new['timestamp'], format='%y-%m-%d, %H:%M')
df_num_new1=df_num_new1.set_index('timestamp').resample('T').agg(stats_list)
df_num_new2=df_num_new2.set_index('timestamp').resample('T').agg(stats_list)
df_num_new3=df_num_new3.set_index('timestamp').resample('T').agg(stats_list)
df_num_new4=df_num_new4.set_index('timestamp').resample('T').agg(stats_list)
print(df_num_new1.head())
print(df_num_new2.head())
print(df_num_new3.head())
print(df_num_new4.head())
#df_num_new = df_num_new.set_index('timestamp')
# df_num_new['agent.hostname']
# df_num_new=df_num_new.groupby(['timestamp','agent.hostname']).agg(stats_list)#.loc[lambda x: x>=0]
# # df_num_new=df_num_new.groupby(['agent.hostname','timestamp']).resample('T').agg(stats_list)#.loc[lambda x: x>=0]
# list(df_num_new.columns)
# df_num_new.head(50)
# df_num_new=df_num.groupby('agent.hostname').resample('T').agg(['std','mean'])
# df_num_new.head()
# grouped = df_num.groupby(['agent.hostname', 'timestamp'])

# In [65]: grouped.aggregate(np.sum)
# df=df_num.groupby(['A', 'B'])
# df_num_new=df_num.groupby('agent.hostname').resample('T').agg(stats_list)
# df_num_new = pd.DataFrame(df_num)
# df_num_new.timestamp=pd.to_datetime(df_num_new.timestamp)
# df_num_new.timestamp.replace(second=0)
# df_num_new.Time=df_num_new.Time.map(lambda x: x.replace(second=0))
# df_num_new.Time=df_num_new.Time.dt.minute+df_num_new.Time.dt.hour+df_num_new.Time.dt.date
# df1['Var_3']=df1['Var_3'].astype(int)
# df_num_new=pd.to_datetime(df_num.Time)
# df_num_new.Time=pd.to_datetime(df_num['timestamp'])
# df1.Time=df1.Time.dt.month+df1.Time.dt.year*100
# df1['Var_3']=df1['Var_3'].astype(int)

# output=df1.groupby(['ID','Product_ID','Time'])
# stats_list=['sum', 'mean','std','median',percentile_25,percentile_75,iqr,percentile_10,percentile_90]

df_num_new=pd.concat([df_num_new1,df_num_new2,df_num_new3,df_num_new4])
df_num_new.head()
# df_num_new.head()

# list(df_num_new.columns)
# df_num_new.head()
# df_num_new.isna().sum()
df_num_new.fillna(0)

# drop constant columns - need to ask ourselves if it is right to do? or it may indicate for an error? 

train=df_num_new.loc[:, (df_num_new != df_num_new.iloc[0]).any()]
train.fillna(0,inplace=True)
train=train.loc[:, (train != train.iloc[0]).any()]
# drop NA columns - need to deal with NA - this is temporary solution for runing the model
#train=train.dropna(1)
# train=train.drop('timestamp',axis=1)

print('train shape ', train.shape, 'original df shape ', df_num_new.shape,'/n',train.head(10))
print(list(train.columns))

import pandas_profiling as pdp
pdp.ProfileReport(train)

#model

clf=IF(behaviour='new',max_samples=75,max_features=20,bootstrap = False, random_state=101,contamination=0.01,verbose=0)

preds=clf.fit_predict(train)
#print(list(preds).count(-1))
metrics_df=pd.DataFrame()
metrics_df['anomaly']=preds
outliers=metrics_df.loc[metrics_df['anomaly']==-1]
outlier_index=list(outliers.index)
#print(outlier_index)
#Find the number of anomalies and normal points here points classified -1 are anomalous
print(metrics_df['anomaly'].value_counts())

print(outlier_index)

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
plt.clf()
plt.close()
# plot anomalies on 3d using pca
pca = PCA(n_components=3)  # Reduce to k=3 dimensions
scaler = StandardScaler()
#normalize the metrics
X = scaler.fit_transform(train)
X_reduce = pca.fit_transform(X)

fig = plt.figure(figsize=(20,10))
ax = fig.add_subplot(111, projection='3d')

ax.set_zlabel("x_composite_3")
# Plot the compressed data points
ax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=10, lw=1, label="inliers",c="green")
# Plot x's for the ground truth outliers
ax.scatter(X_reduce[outlier_index,0],X_reduce[outlier_index,1], X_reduce[outlier_index,2],lw=2, s=60, marker="x", c="red", label="outliers")
# for angle in range(0, 360):
#     ax.view_init(10, 10)
#     plt.draw()
#     plt.pause(10)
ax.legend()
plt.show()



from sklearn import preprocessing
data_scaled = pd.DataFrame(preprocessing.scale(train),columns = train.columns) 
data_scaled.head(10)
pca = PCA(n_components=3)
pca.fit_transform(data_scaled)
export_data=pd.DataFrame(pca.components_,columns=data_scaled.columns,index = ['PC-1','PC-2','PC-3'])
# exporting PCA components
from google.colab import files
export_data.to_csv('export_data.csv') 

print(pca.explained_variance_ratio_)

# ## need to set to give the feature name
# print('pc1 max 3 ',export_data['PC-1'].max(3)
# export_data['PC-2'].max(3)
# export_data['PC-3'].max(3)


#data_scaled = pd.DataFrame(train.StandardScaler(),columns = train.columns)
#data_scaled=pd.DataFrame(X_reduce,columns=(train.columns))
# print (pd.DataFrame(pca.components_,columns=data_scaled.columns,index = ['PC-1','PC-2','PC-3']))

# print (pd.DataFrame(pca.components_,columns=X_reduce.columns,index = ['PC-1','PC-2','PC-3']))

# Set data
df = pd.DataFrame(X_reduce)
df.head()
df.loc[106]

# Libraries
import matplotlib.pyplot as plt
import pandas as pd
from math import pi

# Set data
df = pd.DataFrame({
'group': ['A','B','C','D'],
'var1': [38, 1.5, 30, 4],
'var2': [29, 10, 9, 34],
'var3': [8, 39, 23, 24],
'var4': [7, 31, 33, 14],
'var5': [28, 15, 32, 14]
})

# number of variable
categories=list(df)[1:]
N = len(categories)

# We are going to plot the first line of the data frame.
# But we need to repeat the first value to close the circular graph:
values=df.loc[0].drop('group').values.flatten().tolist()
values1=df.loc[1].drop('group').values.flatten().tolist()
values += values[:1]
values1 += values1[:1]
values

# What will be the angle of each axis in the plot? (we divide the plot / number of variable)
angles = [n / float(N) * 2 * pi for n in range(N)]
angles += angles[:1]

# Initialise the spider plot
plt.figure(figsize=(20,10))
ax = plt.subplot(111, polar=True)

# Draw one axe per variable + add labels labels yet
plt.xticks(angles[:-1], categories, color='grey', size=8)

# Draw ylabels
ax.set_rlabel_position(0)
plt.yticks([10,20,30], ["10","20","30"], color="grey", size=7)
plt.ylim(0,40)

# Plot data
ax.plot(angles, values, linewidth=1, linestyle='solid')
ax.plot(angles, values1, linewidth=1, linestyle='solid')
# Fill area
ax.fill(angles, values, 'b', alpha=0.1)
ax.fill(angles, values1, 'r', alpha=0.1)
plt.show()

# Set data
df = pd.DataFrame(X_reduce)

# number of variable
categories=list(df)
N = len(categories)

# We are going to plot the first line of the data frame.
# But we need to repeat the first value to close the circular graph:
# values=df.loc[0].drop('group').values.flatten().tolist()
# values1=df.loc[106].drop('group').values.flatten().tolist()
values=df.loc[0].values.flatten().tolist()
values1=df.loc[106].values.flatten().tolist()
values += values[:1]
values1 += values1[:1]
values

# What will be the angle of each axis in the plot? (we divide the plot / number of variable)
angles = [n / float(N) * 2 * pi for n in range(N)]
angles += angles[:1]
print(angles,float(N))
# Initialise the spider plot
plt.figure(figsize=(20,10))
ax = plt.subplot(111, polar=True)

# Draw one axe per variable + add labels labels yet
plt.xticks(angles[:-1], categories, color='grey', size=8)

# Draw ylabels
ax.set_rlabel_position(0)
plt.yticks([10,20,30], ["10","20","30"], color="grey", size=7)
plt.ylim(0,40)

# Plot data
ax.plot(angles, values, linewidth=1, linestyle='solid')
ax.plot(angles, values1, linewidth=1, linestyle='solid')
# Fill area
ax.fill(angles, values, 'b', alpha=0.1)
ax.fill(angles, values1, 'r', alpha=0.1)

# df_category_numeric_index=pd.DataFrame()
# df_category_numeric=pd.DataFrame()
# for column in X[col_category]:
#   df_category_numeric_index[column]=pd.factorize(X[col_category][column])
#   df_category_numeric[column]=pd.factorize(X[col_category][column])[0]